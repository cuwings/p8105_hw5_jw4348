---
title: "p8105_hw5_jw4348"
author: "Jingyu Wang"
output: github_document
date: "2023-11-15"
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

For this problem, we are interested in data gathered and made public by _The Washington Post_ on homicides in 50 large U.S. cities. The code chunk below imports and cleans the data.

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>% 
  filter(city_state != "Tulsa, AL") 
```

The resulting dataframe has `r nrow(homicide_df)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. In cleaning, I created a `city_state` variable that includes both city and state, and a `resolution` variable to indicate whether the case was closed by arrest. I also excluded one entry in Tulsa, AL, which is not a major US city and is most likely a data entry error. 

In the next code chunk, I group within cities and summarize to produce the total number of homicides and the number that are solved. 

```{r}
city_homicide_df = 
  homicide_df %>% 
  select(city_state, disposition, resolution) %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```

Focusing only on Baltimore, MD, I can use the `prop.test` and `broom::tidy` functions to obtain an estimate and CI of the proportion of unsolved homicides in that city. The table below shows those values.

```{r}
bmore_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_total)) 

broom::tidy(bmore_test) %>% 
  knitr::kable(digits = 3)
```

Building on this code, I can use functions in the `purrr` package to obtain estimates and CIs for the proportion of unsolved homicides in each city in my dataset. The code below implements this analysis. 

```{r}
test_results = 
  city_homicide_df %>% 
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
  mutate(city_state = fct_reorder(city_state, estimate))
```

Finally, I make a plot showing the estimate (and CI) of the proportion of unsolved homicides in each city.

```{r}
test_results %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This figure suggests a very wide range in the rate at which homicides are solved -- Chicago is noticeably high and, given the narrowness of the CI, likely is the location of many homicides. 

## Problem 2

I will create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time.

#### Start with a dataframe containing all file name.Iterate over file names and read in data for each subject and saving the result as a new variable in the dataframe
```{r}
df_study = 
  tibble(
    file = list.files("data/problem_1/"),
    path = str_c("data/problem_1/", file)
        ) |> 
    mutate(data = map(path, read_csv)) |>
    unnest()
df_study
```

#### And then I tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary

```{r}
tidy_df = 
  df_study |> 
  mutate(
    file = str_remove(file, ".csv"),
        ) |>
  separate(file, into = c("arm", "subject_id"), sep = "_") |>
  pivot_longer(
    cols = week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observations"
              ) |>
  select(subject_id, arm, week, observations)
tidy_df
```

#### Next I will make a spaghetti plot showing observations on each subject over time

```{r}
tidy_df |> 
  ggplot(aes(x = week, y = observations, group = subject_id, color = arm)) +
  geom_point() +
  geom_path() +
  facet_grid( ~ arm) +
  labs(title = "Observations on Each Subject over Time",
       x = "Week",
       y = "Observations") 
```

#### Finally, I will comment on differences between groups.
- In the control arm:
  - The observations fluctuate without a clear overall trend.
  There is no consistent upward or downward pattern in observations from week 1 to 8.

- In the experiment arm:
  - The overall trend in observations is upward over time.
  - Unlike the control group, there is a noticeable increase in observations from week 1 to 8.

- Therefore, the difference trends between groups suggests a potential difference in the response to treatment between the control and experiment groups, with the experiment group showing a more systematic increase in observations over time.

## Problem 3

#### First I will set the design elements
```{r}
sim_test = function(n = 30, mu, sigma = 5) {
  
  x_vec = tibble(
    x = rnorm(n = n, mean = mu, sd = sigma),
  )
  
t_test_result = x_vec |>
    summarize(
      mu_hat = t.test(x_vec,conf.level = 0.95) |>
        broom::tidy() |>
        pull(estimate),
      p_value = t.test(x_vec,conf.level = 0.95) |>
        broom::tidy() |>
        pull(p.value)
    )
}

```

#### Then I will iterate by setting μ=0 and generate 5000 datasets from the model

```{r}
output = vector("list", length = 5000)

for (i in 1:5000) {
  
  output[[i]] = sim_test(mu=0)
  
}

sim_results = bind_rows(output)

sim_results
```

#### Then I will repeat the above for μ={1,2,3,4,5,6}

```{r, cache=TRUE}
sim_result = tibble(
  mu = c(1,2,3,4,5,6)
) |> 
  mutate(
    output_lists = map(mu, ~rerun(5000,sim_test(30,.x,5))),
    estimate_df = map(output_lists, bind_rows)
        ) |>
  select(-output_lists) |> 
  unnest(estimate_df)

sim_result
```

#### Next I will include 0 into μ={1,2,3,4,5,6} for plot

```{r, cache=TRUE}
sim_result_plot = tibble(
  mu = c(0,1,2,3,4,5,6)
) |> 
  mutate(
    output_lists = map(mu, ~rerun(5000,sim_test(30,.x,5))),
    estimate_df = map(output_lists, bind_rows)
        ) |>
  select(-output_lists) |> 
  unnest(estimate_df)

sim_result_plot
```

#### Then I will make a plot showing the proportion of times the null was rejected.

```{r}
sim_result_plot |> 
  mutate(
    decision = case_when(
      p_value < 0.05 ~ "reject",
      p_value >= 0.05 ~ "fail to reject"
                        )
        ) |> 
  group_by(mu) |> 
  summarise(
    all_decision = n(),
    reject = sum(decision == "reject")
  ) |> 
  mutate(
    power = reject/all_decision
         ) |> 
  ggplot(aes(x = mu, y = power, color = mu)) +
  geom_point() +
  geom_line() +
  labs(
    x = "True means",
    y = "Power",
    title = "Plot: Power of Test")

sim_result_plot
```

- We can see from the graph, the power increases as effect size increases. The association between effect size and power is positive.
- Larger effect sizes make differences between groups more noticeable, leading to a greater likelihood of detecting a significant effect.

#### Then I will make another plot showing the average estimate of μ̂ on the y axis and the true value of μ on the x axis.

```{r}
sim_result_plot |> 
  mutate(
    decision = case_when(
      p_value < 0.05 ~ "reject",
      p_value >= 0.05 ~ "fail to reject"
                        )
        ) |>
  group_by(mu) |> 
  summarise(
    average_mu_hat = mean(mu_hat)
  ) |> 
  ggplot(aes(x = mu , y = average_mu_hat, color = mu )) +
  geom_point() +
  geom_line() +
  labs(
     x = "True Means",
     y = "Average Mean Estimate",
     title = "Average Estimtaed Mean vs. True means"
  ) 

sim_result_plot
```

#### Then I will make a second plot the average estimate of μ̂ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis.

```{r}
sim_result_plot |> 
  mutate(
    decision = case_when(
      p_value < 0.05 ~ "reject",
      p_value >= 0.05 ~ "fail to reject"
                        )
        ) |>
  group_by(mu) |> 
  filter(decision == "reject") |>
  summarise(
    average_mu_hat = mean(mu_hat)
  ) |> 
  ggplot(aes(x = mu , y = average_mu_hat, color = mu )) +
  geom_point() +
  geom_line() +
    labs(
     x = "True Means",
     y = "Average Mean Estimate of Reject the Null",
     title = "Average Estimtaed Mean of rejection vs. True means"
  ) 
  
sim_result_plot
```

- At the beginning of the graph, around the effect size before 4, the sample average of μ̂ across tests for which the null is rejected does not approximatly equal to the true value of μ. 
- However, when effect size is equal and greater than 4, the sample average of μ̂ across tests for which the null is rejected approximately equal to the true value of μ.
- The reason can be the power of a statistical test to detect a true effect is influenced by both the effect size and sample size. A smaller effect size makes it challenging to detect a significant difference, especially with a fixed small sample size, as the signal from the difference is weaker. Larger effect sizes, on the other hand, provide a more noticeable and substantial difference, enhancing the test's ability to identify significant departures from the null hypothesis. As the effect size increases, so does the statistical power, reflecting the test's increased likelihood of correctly rejecting a false null hypothesis when a true effect is present. 